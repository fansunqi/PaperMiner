{
  "code_links": "None",
  "tasks": [
    "None"
  ],
  "datasets": [
    "None"
  ],
  "methods": [
    "None"
  ],
  "results": [
    "None"
  ],
  "raw_response": "{\n    \"code_links\": [None],\n    \"tasks\": [\"Partial Differential Equations (PDEs) transfer learning\"],\n    \"datasets\": [None],\n    \"methods\": [\"Transferable neural feature spaces construction\", \"Re-parameterization of hidden neurons\", \"Auxiliary functions for feature space tuning\"],\n    \"results\": [\"Improved transferability\", \"Superior accuracy (mean squared error several orders of magnitude smaller than state of the art)\"]\n}",
  "paper_id": "63d7352390e50fcafda303f5",
  "title": "TransNet: Transferable Neural Networks for Partial Differential\n  Equations",
  "abstract": "  Transfer learning for partial differential equations (PDEs) is to develop a pre-trained neural network that can be used to solve a wide class of PDEs. Existing transfer learning approaches require much information of the target PDEs such as its formulation and/or data of its solution for pre-training. In this work, we propose to construct transferable neural feature spaces from purely function approximation perspectives without using PDE information. The construction of the feature space involves re-parameterization of the hidden neurons and uses auxiliary functions to tune the resulting feature space. Theoretical analysis shows the high quality of the produced feature space, i.e., uniformly distributed neurons. Extensive numerical experiments verify the outstanding performance of our method, including significantly improved transferability, e.g., using the same feature space for various PDEs with different domains and boundary conditions, and the superior accuracy, e.g., several orders of magnitude smaller mean squared error than the state of the art methods. "
}