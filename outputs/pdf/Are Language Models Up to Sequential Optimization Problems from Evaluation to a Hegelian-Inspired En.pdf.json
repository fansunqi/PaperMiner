{
  "code_links": "None",
  "tasks": [
    "None"
  ],
  "datasets": [
    "None"
  ],
  "methods": [
    "None"
  ],
  "results": [
    "None"
  ],
  "raw_response": "```json\n{\n    \"code_links\": [None],\n    \"tasks\": [\"Sequential Optimization Problems\"],\n    \"datasets\": [None],\n    \"methods\": [\"WorldGen\", \"ACE\"],\n    \"results\": [None]\n}\n```",
  "title": "Are Language Models Up to Sequential Optimization Problems from Evaluation to a Hegelian-Inspired En.pdf",
  "abstract": "Large Language Models (LLMs) have demon- strated impressive capabilities across numerous fields, presenting an opportunity to revolutionize optimization problem-solving, a crucial, ubiqui- tous, and complex domain. This paper explores the proficiency of LLMs in handling Sequential Optimization Problems (SOPs). We introduce WorldGen, a dynamic framework for generating unseen SOPs with controllable complexities, to evaluate LLM performance. Our initial observa- tions reveal that while LLMs perform well on simple SOPs, their performance significantly de- grades with increased complexity. Motivated by this, we revisit philosophical hypotheses on rea- soning to enhance LLM performance. Inspired by the influential framework of Hegelian Dialec- tics, we propose ACE, demonstrating how the performance of LLMs in SOP contexts can be significantly improved without any retraining or further fine-tuning. 1. Introduction Setting the Context: Optimization is fundamental to decision-making across diverse domains such as engineer- ing, science, economics, healthcare, and even nature. The essence of decision-making lies in choosing the best op- tion from a set of alternatives, driven by objectives such as efficient resource allocation, cost minimization, profit maximization, or performance enhancement of systems and infrastructures (Chong & Zak, 2013). However, solving optimization problems is often intricate, requiring special- ized expertise and addressing practical challenges like high dimensionality, nonlinearity, and the dynamic, stochastic nature of real-world environments (Datta et al., 2019). Con- sequently, there is a continuous quest across various fields to simplify the optimization process. A New Opportunity: Simultaneously, Large Language Models (LLMs), such as GPT-4 (OpenAI, 2023a), have shown impressive capabilities in both general assistance and specialized domains, including mathematics, coding, and law (Bubeck et al., 2023). These advancements present a compelling opportunity to revolutionize our approach to solving optimization problems by leveraging LLMs to auto- mate solution processes. This emerging potential prompts a fundamental scientific inquiry: How proficient are current LLMs in handling optimization problems? Assessing LLMs in a New Context: Although various benchmarks exist to evaluate LLM performance in gen- eral and specialized tasks (e.g., coding, mathematics), their capabilities in solving optimization problems, particularly Sequential Optimization Problems (SOPs), remain under- explored. SOPs involve making a series of decisions over time, where each decision affects subsequent options and outcomes, creating a complex web of interdependencies. Different challenges contribute to this issue, primarily the need for defining a representative set of SOPs and ensuring that observed performance is not influenced by data contam- ination or prior exposure during the LLMs\u2019 training (Dong et al., 2024; Mialon et al., 2023; Chen, 2023). On-Demand SOP Generation: Motivated by these oppor- tunities and challenges, in the first half of this paper we aim to addresse this research question: How can we eval- uate the performance of LLMs in SOPs? We introduce a straightforward yet effective framework, WorldGen, capa- ble of generating unseen SOPs with controllable complex- ities on demand. WorldGen contrasts with most existing static benchmarks (e.g., MMLU (Hendrycks et al., 2021), GLUE (Wang et al., 2018), SuperGLUE (Wang et al., 2019), GSM8k (Cobbe et al., 2021), etc.) which become obsolete as LLMs evolve (Mialon et al., 2023). Utilizing this dy- namic framework, we have made two key observations: (1) For relatively simple optimization tasks, with a single global maximum and no local maxima (i.e., simple surfaces and scenarios), current LLMs can solve them efficiently. (2) As the complexity of the optimization problems increases, the performance of LLMs degrades significantly and becomes unsatisfactory. Improving LLMs with Roots in Philosophy: Inspired by 1 arXiv:2502.02573v1 [cs.CL] 4 Feb 2025"
}